\chapter{Implementation}

\section{Construction of Constrain Matrix}
Before solving the input encoding problem the given problem specification must be transformed into a constrain matrix. This constrain matrix is further called $A$. Each row $i$ of $A_{i,j}$ describes a constrain on the resulting encoding. Furthermore, each column $j$ of $A_{i,j}$ assigned to a encoded symbol. For example the row $(1,1,0,0)$ means that symbols $3, 4$ can be encode together so that the resulting super cube does not include symbols $1, 2$. This constrain would be equal to the row $(0,0,1,1)$. The reason for this that a dichotomy describes a bipartition of a set, but for encoding problems only the relation between the elements in both sets are important. Therefore, the partition $A_0:{1,2} A_1:{3,4}$ and the partition $B_0:{3,4} B_1:{1,2}$ are inverse partitions but the relations are all equal. In $A$ and $B$ the elements $1, 2$ are combined in a set and are not combined with $3, 4$. A row describes also a bipartition of all symbols.

The given problem specification used symbolic names for state and binary notation including don't cares for input and output vectors. The constrain matrix $A$ is a result of the minimal symbolic cover. Because of the binary notation of the input and output vectors we transformed the symbolic problem into a binary coded cover problem. We used the positional cube notation to deal with input and output binary notation including don't cares. For the symbolic state names we used a one hot positional encoding. 

A covering super cube can for a set of positional cube notation vectors is computed by AND-conjunction all vectors. The resulting cube must be tested for validity. This is given when all cubes are valid (not equals $2'b00$).

\subsection{Minimal Cover Algorithm}
For implementation we implemented an iterative approach. The algorithm terminates when no further improvements are possible. This is when all combination of entries are tested. When a optimization is possible the two vectors that are combined are removed and a new vector covering both is included. When this happens all combinations of entries are tried again until termination.

From the set of resulting super cubes for the states constrain matrix $A$ can be constructed. Only the entries that actual constrain the problem are for interested, therefor entries including all symbols as symbolic implicant or entries that include only one symbolic literal as implicant can be removed. Only entries that portion the symbolic state into a relation of one symbol can be combined with others not include other symbol, like the given example, are for interested. 

\section{Generation of Root Dichotomies}
The generation of all root dichotomies is straight forward implementation of a sequential generator. This generator uses all rows of the constrain matrix and computes all resulting root dichotomies for each row. The number of root dichotomies are equal to the number of symbols assigned a zero in the constrain row.

\section{Generation of Prime Dichotomies Candidates}
As stated before to find a exact solution all candidates prime dichotomies must be checked. A candidate is a dichotomy where a symbols are portioned in both sets. We used a long vector with positional symbol notation where each bit is assigned to a symbol. Just by iterating all possible values all prime dichotomies candidates are reached. The inverse property of dichotomies regarding encoding is used to reduce set of candidates to the half. Because all values in the lower half have a value inverse equal in the upper half. For example a positional symbol notation vector for 5 five symbols of $2'b00110$ is equal to $2'b11001$.

\section{Prime Dichotomies Coverage Table}
Before we solve the encoding problem we need to compute which prime dichotomy covers which root dichotomies. For this we generate a table of lookup vectors. Each vector describes the cover relation between a prime dichotomy and all root dichotomies, by using again a positional notation where a bit describes the coverage.

\section{Cover Root Dichotomies with Primes}
Using the before created table it is now possible to look for a minimal set of prime dichotomies that cover all root dichotomies. We start with the small set possible of one prime dichotomy. When no solution is found the search space is increased by one additional element. The combination of elements for the minimal set of prime dichotomies is generator that iterate all combination without repetition. For the $k$ round $\left( n\atop k \right)$ for $n$ table entries are possible.

\section{Optimizations}

The complexity to find an exact solution scales with $\left( n\atop k \right)$, where $n$ is the size of the coverage table and $k$ is the amount of lines that are combined to find a solution. One way to shorten runtime is to reduce the table size. Another way is to find a faster algorithm to search the table, but this comes at the cost of non-exact solutions. It is also desirable to parallelize the process of searching the coverage table to utilize multiple processors.

\subsection{Coverage Table minimization}

Some entries in the coverage table can be removed without lowering the result quality. A line $l_1$ in the table is eligible for removal if it is covered by another line $l_2$ in the table. $l_1$ is covered by $l_2$ if all bits that are set in $l_1$ are also set in $l_2$. Reducing the table size in this way takes additional time and is more expensive for large tables, but it can reduce table sizes drastically. Evaluation has shown that large coverage tables can be reduced by a factor of 2.

\subsection{Parallelization}

The search for a solution in the coverage table can be parallelized by partitioning the set of all possible combinations of table entries. After the partitions are formed they are assigned to a thread that combines the table entries and checks whether a valid solution was found. When all combinations in the partition have been checked the thread requests another partition that is then checked for a solution.

\subsection{Non exact solution}

Large problems that result in big coverage tables take a long time to find an exact solution. The runtime becomes so long that it is impractical to search for an exact solution. Because of this we also implemented an algorithm that searches for a non exact solution. It works in the following way:
\begin{enumerate}
\item Select the entry from the coverage table that has the most bits set. If a valid solution is already found abort the search.
\item Try every entry from the coverage table by combining it with the previously selected entry/entries and save the number of additional bits that are set after combining the entries.
\item Select the entry that adds the most set bits to the result. If a valid solution is found abort the process. If not repeat step 2 and 3 until a solution is found.
\end{enumerate}

This non exact approach results in drastically reduced runtime but will typically generate worse results than the exact approach. Both algorithms are compared in chapter \ref{cha:evaluation}.